{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdAlS0bCnJVhhG3EH046GA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SohaHussain/LangChain/blob/main/Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieval is important at query time, when we have a query that comes in and we want to retrieve the most relavant splits."
      ],
      "metadata": {
        "id": "5onIUCq3NJP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Maximum Marginal Relevance"
      ],
      "metadata": {
        "id": "89qc7qi8Ovjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   we may not always want to choose the most similar response\n",
        "*   the idea behind MMR is, we send a query in and then we initially get back a set responses with 'fetch_k' being the parameter to control the number of responses.\n",
        "\n"
      ],
      "metadata": {
        "id": "A1q2Gp5rO1xc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####LLM Aided Retrieval"
      ],
      "metadata": {
        "id": "W3GzKe3-Tw9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Another type of retrieval we can do is what we call self-query. So\n",
        "this is useful when you get questions that aren't solely\n",
        "about the content that you want to look\n",
        "up semantically, but also include some mention of some\n",
        "metadata that you want to do a filter on. What we can do is we can use a language model itself to\n",
        "split that original question into two separate things,\n",
        "a filter and a search term. Most vector stores support a\n",
        "metadata filter. So you can easily filter\n",
        "records based on metadata.\n",
        "\n"
      ],
      "metadata": {
        "id": "eFxTasYCTzqe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Compression"
      ],
      "metadata": {
        "id": "rkW7VuWkUXQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This\n",
        "can be useful to really pull out only\n",
        "the most relevant bits of the retrieved passages. For\n",
        "example, when asking a question, you get back the whole\n",
        "document that was stored, even if only the first one or two sentences\n",
        "are the relevant parts. With compression, you can then run\n",
        "all those documents through a language model and extract the most relevant\n",
        "segments and then pass only the most relevant segments\n",
        "into a final language model call. This\n",
        "comes at the cost of making more calls\n",
        "to the language model, but it's also really good for focusing the final\n",
        "answer on only the most important things. And so it's a bit\n",
        "of a tradeoff."
      ],
      "metadata": {
        "id": "1KTNqT-SUwBk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv6EcTpKM10r",
        "outputId": "823513c2-da1b-49c8-ddef-d47d03166952"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.300-py3-none-any.whl (1.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.7 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.0-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.38 (from langchain)\n",
            "  Downloading langsmith-0.0.40-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.300 langsmith-0.0.40 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMIcMJRAVUqB",
        "outputId": "5f7ed055-b6fd-4aed-9998-a6aac6854cd2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "5pxJNuWWVcin"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "open_ai_key = '##'\n",
        "openai.api_key = open_ai_key"
      ],
      "metadata": {
        "id": "VqJH_XeDVhUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Similarity Search"
      ],
      "metadata": {
        "id": "T3_jk9I2Yh1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "persist_directory = 'docs/chroma/'"
      ],
      "metadata": {
        "id": "_PKtxbboVkhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = OpenAIEmbeddings()\n",
        "vectordb = Chroma(\n",
        "    persist_directory=persist_directory,\n",
        "    embedding_function=embedding\n",
        ")"
      ],
      "metadata": {
        "id": "IqwFXPyUYl36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectordb._collection.count()) # no. of documents"
      ],
      "metadata": {
        "id": "A8fptDeaYqEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"\"\"The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\"\"\",\n",
        "    \"\"\"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\"\"\",\n",
        "    \"\"\"A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\"\"\",\n",
        "]"
      ],
      "metadata": {
        "id": "rll31AgFYyvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smalldb = Chroma.from_texts(texts, embedding=embedding) # creating a small database"
      ],
      "metadata": {
        "id": "qJA2gN-GZTQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Tell me about all-white mushrooms with large fruiting bodies\"\n",
        "smalldb.similarity_search(question, k=2) # returns 2 most relevant docs"
      ],
      "metadata": {
        "id": "eU0COXJqZbiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smalldb.max_marginal_relevance_search(question,k=2, fetch_k=3) # using MMR"
      ],
      "metadata": {
        "id": "txYyuESUaMeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Addressing Diversity: Maximum marginal relevance"
      ],
      "metadata": {
        "id": "mqMU_l6IetwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Maximum marginal relevance` strives to achieve both relevance to the query *and diversity* among the results."
      ],
      "metadata": {
        "id": "gv1sH8Uuez0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what did they say about matlab?\"\n",
        "docs_ss = vectordb.similarity_search(question,k=3)"
      ],
      "metadata": {
        "id": "qn_Fl6Amb1kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_ss[0].page_content[:100]"
      ],
      "metadata": {
        "id": "MilY3Vu0e7BJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_ss[1].page_content[:100]"
      ],
      "metadata": {
        "id": "d5vF81n2fASU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_mmr = vectordb.max_marginal_relevance_search(question,k=3) # using MMR\n",
        "docs_mmr[0].page_content[:100]"
      ],
      "metadata": {
        "id": "jE68LW6CfDCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Addressing Specificity: working with metadata"
      ],
      "metadata": {
        "id": "CcYOWp8JfS2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "many vectorstores support operations on `metadata`.\n",
        "`metadata` provides context for each embedded chunk."
      ],
      "metadata": {
        "id": "Uv_aRfOSfZd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what did they say about regression in the third lecture?\"\n",
        "docs = vectordb.similarity_search(\n",
        "    question,\n",
        "    k=3,\n",
        "    filter={\"source\":\"docs/cs229_lectures/MachineLearning-Lecture03.pdf\"}\n",
        ")"
      ],
      "metadata": {
        "id": "RrDLnmGsfJ-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for d in docs:\n",
        "    print(d.metadata)"
      ],
      "metadata": {
        "id": "-RzvZ-UMfjfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Addressing Specificity: working with metadata using self-query retriever"
      ],
      "metadata": {
        "id": "t1q8tkm4h6o6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we often want to infer the metadata from the query itself.\n",
        "\n",
        "To address this, we can use `SelfQueryRetriever`, which uses an LLM to extract:\n",
        "\n",
        "1. The `query` string to use for vector search\n",
        "2. A metadata filter to pass in as well\n",
        "\n",
        "Most vector databases support metadata filters, so this doesn't require any new databases or indexes."
      ],
      "metadata": {
        "id": "rPwWvpzniBcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "from langchain.chains.query_constructor.base import AttributeInfo"
      ],
      "metadata": {
        "id": "J5olRvmBfx65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_field_info = [\n",
        "    AttributeInfo(\n",
        "        name=\"source\",\n",
        "        description=\"The lecture the chunk is from, should be one of `docs/cs229_lectures/MachineLearning-Lecture01.pdf`, `docs/cs229_lectures/MachineLearning-Lecture02.pdf`, or `docs/cs229_lectures/MachineLearning-Lecture03.pdf`\",\n",
        "        type=\"string\",\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name=\"page\",\n",
        "        description=\"The page from the lecture\",\n",
        "        type=\"integer\",\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "xtoUSXaDiGJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_content_description = \"Lecture notes\" # what the document is about\n",
        "llm = OpenAI(temperature=0)\n",
        "retriever = SelfQueryRetriever.from_llm(\n",
        "    llm,\n",
        "    vectordb,\n",
        "    document_content_description,\n",
        "    metadata_field_info,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "M5cM_jFPjCrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what did they say about regression in the third lecture?\"\n",
        "docs = retriever.get_relevant_documents(question)\n",
        "for d in docs:\n",
        "    print(d.metadata)"
      ],
      "metadata": {
        "id": "iiHY_nxQjPen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Compression"
      ],
      "metadata": {
        "id": "xwnKjtu7kdKt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another approach for improving the quality of retrieved docs is compression.\n",
        "\n",
        "Information most relevant to a query may be buried in a document with a lot of irrelevant text.\n",
        "\n",
        "Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
        "\n",
        "Contextual compression is meant to fix this."
      ],
      "metadata": {
        "id": "22t5L43sk6NZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor"
      ],
      "metadata": {
        "id": "oaPkrWkQkMZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pretty_print_docs(docs):\n",
        "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))"
      ],
      "metadata": {
        "id": "0Okbe5Fnk9z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap our vectorstore\n",
        "llm = OpenAI(temperature=0)\n",
        "compressor = LLMChainExtractor.from_llm(llm)"
      ],
      "metadata": {
        "id": "E8sjdgc1n1eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=vectordb.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "jlP4Eg25n31b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what did they say about matlab?\"\n",
        "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
        "pretty_print_docs(compressed_docs)"
      ],
      "metadata": {
        "id": "d16IN7oVn6z5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}