{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnB9ahcy46PKA06KMpA5IT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SohaHussain/LangChain/blob/main/Embeddings_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embeddings is a way of representing data as points in space where the locations are semantically meaningful. The word 'semantic' just means meanings, where the location captures something about the meaning of a piece of text."
      ],
      "metadata": {
        "id": "rXwVp8C_OK9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modern Embeddings uae a transformer neural network  to compute a context aware representation of each word then take an average of the context aware representations."
      ],
      "metadata": {
        "id": "Cn4DPwuKRJW9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc0XEVse0qFS",
        "outputId": "58522656-da21-49b6-d7a9-aea86525e822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.10/dist-packages (1.42.1)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.11.1)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.23.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.20.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (23.2)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.12.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.12.1)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.0.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.62.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.31.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.60.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.0)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.0)\n",
            "Requirement already satisfied: numpy<2,>=1.14 in /usr/local/lib/python3.10/dist-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.25.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.5.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hywZaq8V16nF",
        "outputId": "8f67a850-a1e3-4a99-ac2d-c7d6ec98b03b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting utils\n",
            "  Downloading utils-1.0.2.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: utils\n",
            "  Building wheel for utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for utils: filename=utils-1.0.2-py2.py3-none-any.whl size=13905 sha256=122b1abb70d8e5d4d9617785a3275e87924d8af2e964c3eb90278b32af9cff55\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/39/f5/9d0ca31dba85773ececf0a7f5469f18810e1c8a8ed9da28ca7\n",
            "Successfully built utils\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from utils import authenticate\n",
        "credentials, PROJECT_ID = authenticate() # Get credentials and project ID"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "ZkxKdTzb1LCj",
        "outputId": "5962bcc9-4722-4298-831c-2174e012eaa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'authenticate' from 'utils' (/usr/local/lib/python3.10/dist-packages/utils/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2c2f8e7b96fc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauthenticate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPROJECT_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauthenticate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Get credentials and project ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'authenticate' from 'utils' (/usr/local/lib/python3.10/dist-packages/utils/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "REGION = 'us-central1'"
      ],
      "metadata": {
        "id": "HRAsFHU-12vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "vertexai.init(project = PROJECT_ID,\n",
        "              location = REGION,\n",
        "              credentials = credentials)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "SRrys_xojs6d",
        "outputId": "0755f66d-6c79-4938-8055-22c33c45c0f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'PROJECT_ID' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-fbb11ebe15b0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvertexai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m vertexai.init(project = PROJECT_ID, \n\u001b[0m\u001b[1;32m      3\u001b[0m               \u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mREGION\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m               credentials = credentials)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PROJECT_ID' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.language_models import TextEmbeddingModel"
      ],
      "metadata": {
        "id": "7ct6LD5Ljv2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = TextEmbeddingModel.from_pretrained(\n",
        "    \"textembedding-gecko@001\")"
      ],
      "metadata": {
        "id": "gBVEZbdQjygk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = embedding_model.get_embeddings(\n",
        "    [\"life\"])"
      ],
      "metadata": {
        "id": "PUYg11_bj0uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = embedding[0].values\n",
        "print(f\"Length = {len(vector)}\")\n",
        "print(vector[:10])"
      ],
      "metadata": {
        "id": "jEY8JkUqj5EY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "tyszyZLMj87U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb_1 = embedding_model.get_embeddings(\n",
        "    [\"What is the meaning of life?\"]) # 42!\n",
        "\n",
        "emb_2 = embedding_model.get_embeddings(\n",
        "    [\"How does one spend their time well on Earth?\"])\n",
        "\n",
        "emb_3 = embedding_model.get_embeddings(\n",
        "    [\"Would you like a salad?\"])\n",
        "\n",
        "vec_1 = [emb_1[0].values]\n",
        "vec_2 = [emb_2[0].values]\n",
        "vec_3 = [emb_3[0].values]"
      ],
      "metadata": {
        "id": "rzAuoX5rkAVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "NY0vexb7l4bH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_bq_query(sql):\n",
        "\n",
        "    # Create BQ client\n",
        "    bq_client = bigquery.Client(project = PROJECT_ID,\n",
        "                                credentials = credentials)\n",
        "\n",
        "    # Try dry run before executing query to catch any errors\n",
        "    job_config = bigquery.QueryJobConfig(dry_run=True,\n",
        "                                         use_query_cache=False)\n",
        "    bq_client.query(sql, job_config=job_config)\n",
        "\n",
        "    # If dry run succeeds without errors, proceed to run query\n",
        "    job_config = bigquery.QueryJobConfig()\n",
        "    client_result = bq_client.query(sql,\n",
        "                                    job_config=job_config)\n",
        "\n",
        "    job_id = client_result.job_id\n",
        "\n",
        "    # Wait for query/job to finish running. then get & return data frame\n",
        "    df = client_result.result().to_arrow().to_pandas()\n",
        "    print(f\"Finished job_id: {job_id}\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "1E0zK1E7kD7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "language_list = [\"python\", \"html\", \"r\", \"css\"]"
      ],
      "metadata": {
        "id": "v9GJCIHyl6eG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "so_df = pd.DataFrame()\n",
        "\n",
        "for language in language_list:\n",
        "\n",
        "    print(f\"generating {language} dataframe\")\n",
        "\n",
        "    query = f\"\"\"\n",
        "    SELECT\n",
        "        CONCAT(q.title, q.body) as input_text,\n",
        "        a.body AS output_text\n",
        "    FROM\n",
        "        `bigquery-public-data.stackoverflow.posts_questions` q\n",
        "    JOIN\n",
        "        `bigquery-public-data.stackoverflow.posts_answers` a\n",
        "    ON\n",
        "        q.accepted_answer_id = a.id\n",
        "    WHERE\n",
        "        q.accepted_answer_id IS NOT NULL AND\n",
        "        REGEXP_CONTAINS(q.tags, \"{language}\") AND\n",
        "        a.creation_date >= \"2020-01-01\"\n",
        "    LIMIT\n",
        "        500\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    language_df = run_bq_query(query)\n",
        "    language_df[\"category\"] = language\n",
        "    so_df = pd.concat([so_df, language_df],\n",
        "                      ignore_index = True)"
      ],
      "metadata": {
        "id": "WRE7dBPkmVxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "PWz2Oi7QnHlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_batches(sentences, batch_size = 5):\n",
        "    for i in range(0, len(sentences), batch_size):\n",
        "        yield sentences[i : i + batch_size]"
      ],
      "metadata": {
        "id": "nuWUDX17nSJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This helper function calls model.get_embeddings() on the batch of data, and returns a list containing the embeddings for each text in that batch.\n",
        "\n",
        "def encode_texts_to_embeddings(sentences):\n",
        "    try:\n",
        "        embeddings = model.get_embeddings(sentences)\n",
        "        return [embedding.values for embedding in embeddings]\n",
        "    except Exception:\n",
        "        return [None for _ in range(len(sentences))]"
      ],
      "metadata": {
        "id": "xX1ivCTznWZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_embeddings = encode_texts_to_embeddings(batch)\n"
      ],
      "metadata": {
        "id": "6RIZ9TPLn05k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### text generation"
      ],
      "metadata": {
        "id": "bIxuVnWgo2sH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.language_models import TextGenerationModel"
      ],
      "metadata": {
        "id": "HY_a0gRLpO38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\") # text generation model for chat conversations use chat-bison model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "a50BbYJ0o5Dz",
        "outputId": "cc88cff2-f9b3-4692-fb4b-2afd7877cca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unable to find your project. Please provide a project ID by:\n- Passing a constructor argument\n- Using vertexai.init()\n- Setting project using 'gcloud config set project my-project'\n- Setting a GCP environment variable\n- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-68f80191f0fa>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgeneration_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextGenerationModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text-bison@001\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# text generation model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vertexai/_model_garden/_model_garden_models.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model_name)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_from_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterface_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mauth_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGoogleAuthError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mauth_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGoogleAuthError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcredential_exception_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vertexai/_model_garden/_model_garden_models.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(interface_class, model_name, publisher_model, tuned_vertex_model)\u001b[0m\n\u001b[1;32m    206\u001b[0m             )\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         model_info = _get_model_info(\n\u001b[0m\u001b[1;32m    209\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mschema_to_class_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minterface_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_INSTANCE_SCHEMA_URI\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minterface_class\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vertexai/_model_garden/_model_garden_models.py\u001b[0m in \u001b[0;36m_get_model_info\u001b[0;34m(model_id, schema_to_class_map, interface_class, publisher_model_res, tuned_vertex_model)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpublisher_model_res\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         publisher_model_res = (\n\u001b[0;32m--> 124\u001b[0;31m             _publisher_models._PublisherModel(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    125\u001b[0m                 \u001b[0mresource_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             )._gca_resource\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/aiplatform/_publisher_models.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, resource_name, project, location, credentials)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \"\"\"\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_resource_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, project, location, credentials, resource_name)\u001b[0m\n\u001b[1;32m    508\u001b[0m             )\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocation\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcredentials\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/aiplatform/initializer.py\u001b[0m in \u001b[0;36mproject\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mproject_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject_not_found_exception_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mproject_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unable to find your project. Please provide a project ID by:\n- Passing a constructor argument\n- Using vertexai.init()\n- Setting project using 'gcloud config set project my-project'\n- Setting a GCP environment variable\n- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"I'm a high school student.Recommend me a programming activity to improve my skills.\""
      ],
      "metadata": {
        "id": "hF17PqK9pMt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generation_model.predict(prompt=prompt).text)"
      ],
      "metadata": {
        "id": "O4v0O3y3qd2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can control the behavior of the language model's decoding strategy by adjusting the temperature, top-k, and top-n parameters."
      ],
      "metadata": {
        "id": "MNxpJSHYvDpP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in neural networks we have raw ouputs called logits and we pass these logits to a softmax function in order to get a probability distribution over classes. Here different classes can be considered the different tokens to be returned to the user. So when we apply temperature what we are doing is that we are taking our softmax function and dividing each of our logits value by the temperature value. So as we decrease the temperature value we are increasing the likelihood of selecting the most probable token."
      ],
      "metadata": {
        "id": "twhVt353wFj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another strategy is top_k, where we sample from the top k most probable tokens. The default value for top_k is 40.\n",
        "You can set top_k to values between 1 and 40."
      ],
      "metadata": {
        "id": "5R3cJywTy4Q3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last strategy is top_p where we can dynamically set the number of tokens to sample from whose cummalative probability is greater than or equal to p. The default value for top_p is 0.95."
      ],
      "metadata": {
        "id": "6KGHhTLwzvwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "top k -> top p -> temperature"
      ],
      "metadata": {
        "id": "MGsvpcya0NcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 20\n",
        "top_p = 0.7"
      ],
      "metadata": {
        "id": "yhuCTz-6qekS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write an advertisement for jackets \\\n",
        "that involves blue elephants and avocados.\""
      ],
      "metadata": {
        "id": "slYd1lBK0WEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = generation_model.predict(\n",
        "    prompt=prompt,\n",
        "    temperature=0.9,\n",
        "    top_k=top_k,\n",
        "    top_p=top_p,\n",
        ")"
      ],
      "metadata": {
        "id": "wgNn61FCXbAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grounding in LLM:\n",
        "\n",
        "\n",
        "*   Access information outside of training data\n",
        "*   Integrate with existing databases and business data\n",
        "*   Mitigate risk of hallucinations\n",
        "\n"
      ],
      "metadata": {
        "id": "ZHnXgAA-ZSC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Measuring similarity using embeddings\n",
        "\n",
        "1.   Euclidean distance(l2 distance)\n",
        "2.   Cosine Similarity\n",
        "3.   Dot Product\n",
        "\n"
      ],
      "metadata": {
        "id": "Kq9MOKnpb3af"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###semantic search"
      ],
      "metadata": {
        "id": "IQmHZjZWcnk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import pairwise_distances_argmin as distances_argmin"
      ],
      "metadata": {
        "id": "YPiefMikXeF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = ['How to concat dataframes pandas']"
      ],
      "metadata": {
        "id": "cDWbVM3Ucsv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_embedding = embedding_model.get_embeddings(query)[0].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "bVZgj3jGcz_0",
        "outputId": "b5f08ba0-c441-49aa-cfb5-187fe5bcc883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'embedding_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7799afd47663>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquery_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'embedding_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cos_sim_array = cosine_similarity([query_embedding],\n",
        "                                  list(so_database.embeddings.values))"
      ],
      "metadata": {
        "id": "YB0JEyRkc3Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_doc_cosine = np.argmax(cos_sim_array)"
      ],
      "metadata": {
        "id": "w8pRW1Hqc7JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_doc_distances = distances_argmin([query_embedding],\n",
        "                                       list(so_database.embeddings.values))[0]"
      ],
      "metadata": {
        "id": "PwhaA7tehVCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"Here is the context: {context}\n",
        "             Using the relevant information from the context,\n",
        "             provide an answer to the query: {query}.\"\n",
        "             If the context doesn't provide \\\n",
        "             any relevant information, \\\n",
        "             answer with \\\n",
        "             [I couldn't find a good match in the \\\n",
        "             document database for your query]\n",
        "             \"\"\""
      ],
      "metadata": {
        "id": "OXyd43fahX7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "t_value = 0.2\n",
        "response = generation_model.predict(prompt = prompt,\n",
        "                                    temperature = t_value,\n",
        "                                    max_output_tokens = 1024)\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "JhJWzPihh-pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scale with approximate nearest neighbor search\n",
        "\n",
        "When dealing with a large dataset, computing the similarity between the query and each original embedded document in the database might be too expensive. Instead of doing that, you can use approximate nearest neighbor algorithms that find the most similar documents in a more efficient way.\n",
        "\n",
        "These algorithms usually work by creating an index for your data, and using that index to find the most similar documents for your queries. In this notebook, we will use ScaNN to demonstrate the benefits of efficient vector similarity search. First, you have to create an index for your embedded dataset."
      ],
      "metadata": {
        "id": "gT6JSgUNs5ku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scann\n",
        "from utils import create_index\n",
        "\n",
        "#Create index using scann\n",
        "index = create_index(embedded_dataset = question_embeddings,\n",
        "                     num_leaves = 25,\n",
        "                     num_leaves_to_search = 10,\n",
        "                     training_sample_size = 2000)"
      ],
      "metadata": {
        "id": "8G78TkDgiBdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"how to concat dataframes pandas\""
      ],
      "metadata": {
        "id": "8cNfbkOjs-jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "query_embedding = embedding_model.get_embeddings([query])[0].values\n",
        "neighbors, distances = index.search(query_embedding, final_num_neighbors = 1)\n",
        "end = time.time()\n",
        "\n",
        "for id, dist in zip(neighbors, distances):\n",
        "    print(f\"[docid:{id}] [{dist}] -- {so_database.input_text[int(id)][:125]}...\")\n",
        "\n",
        "print(\"Latency (ms):\", 1000 * (end - start))"
      ],
      "metadata": {
        "id": "FTqU0hAltBet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "query_embedding = embedding_model.get_embeddings([query])[0].values\n",
        "cos_sim_array = cosine_similarity([query_embedding], list(so_database.embeddings.values))\n",
        "index_doc = np.argmax(cos_sim_array)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"[docid:{index_doc}] [{np.max(cos_sim_array)}] -- {so_database.input_text[int(index_doc)][:125]}...\")\n",
        "\n",
        "print(\"Latency (ms):\", 1000 * (end - start))"
      ],
      "metadata": {
        "id": "2ZVqT4owtEG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t9PE2WATtHhB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}